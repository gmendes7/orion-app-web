import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.57.0";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers":
    "authorization, x-client-info, apikey, content-type",
};

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { messages: conversationHistory, userId, conversationId } = await req.json();

    const lovableApiKey = Deno.env.get("LOVABLE_API_KEY");
    if (!lovableApiKey) {
      throw new Error("LOVABLE_API_KEY n√£o configurada");
    }

    const supabaseUrl = Deno.env.get("SUPABASE_URL")!;
    const supabaseServiceKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    console.log("üöÄ Enviando mensagem para Lovable AI Gateway:", {
      messageCount: conversationHistory.length,
      model: "google/gemini-2.5-flash",
    });

    // Get the last user message to generate embedding
    const lastUserMessage = conversationHistory
      .slice()
      .reverse()
      .find((msg: any) => msg.role === "user");

    let contextualMemory = "";

    if (lastUserMessage && userId) {
      try {
        console.log("üîç Buscando mem√≥ria contextual...");

        // Generate embedding for the user query
        const embeddingResponse = await fetch("https://ai.gateway.lovable.dev/v1/embeddings", {
          method: "POST",
          headers: {
            Authorization: `Bearer ${lovableApiKey}`,
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            model: "text-embedding-3-small",
            input: lastUserMessage.content,
          }),
        });

        if (embeddingResponse.ok) {
          const embeddingData = await embeddingResponse.json();
          const queryEmbedding = embeddingData.data[0].embedding;

          // Search for similar messages
          const { data: similarMessages, error: searchError } = await supabase.rpc(
            "search_similar_messages",
            {
              query_embedding: queryEmbedding,
              user_id_param: userId,
              match_threshold: 0.7,
              match_count: 3,
              exclude_conversation_id: conversationId,
            }
          );

          if (!searchError && similarMessages && similarMessages.length > 0) {
            console.log(`‚úÖ Encontradas ${similarMessages.length} mensagens relevantes`);
            
            const memoryContext = similarMessages
              .map((msg: any) => {
                const role = msg.is_user ? "Usu√°rio" : "Orion";
                return `[${role}]: ${msg.content}`;
              })
              .join("\n\n");

            contextualMemory = `\n\nüìö MEM√ìRIA CONTEXTUAL (conversas anteriores relevantes):\n${memoryContext}\n\n`;
          }
        }
      } catch (memoryError) {
        console.error("‚ö†Ô∏è Erro ao buscar mem√≥ria contextual:", memoryError);
        // Continue sem mem√≥ria contextual se houver erro
      }
    }

    const response = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${lovableApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "google/gemini-2.5-flash", // Modelo padr√£o Lovable AI
        messages: [
          {
            role: "system",
            content: `Voc√™ √© O.R.I.√ñ.N (Observational & Responsive Intelligence √ñdyssey Navigator), uma IA avan√ßada.${contextualMemory}

üéØ **Miss√£o Principal:**
Fornecer respostas precisas, √∫teis e naturais, criando uma experi√™ncia conversacional fluida e agrad√°vel.

üìù **Diretrizes de Comunica√ß√£o:**
‚Ä¢ **Clareza**: Use par√°grafos curtos e objetivos (m√°x 3-4 linhas cada)
‚Ä¢ **Formata√ß√£o**: Utilize markdown para organizar respostas:
  - **Negrito** para conceitos importantes
  - *It√°lico* para √™nfase
  - Listas numeradas ou com bullets para m√∫ltiplos pontos
  - Blocos de c√≥digo quando relevante
‚Ä¢ **Tom**: Natural e amig√°vel, adaptando-se ao contexto (casual, t√©cnico ou formal)
‚Ä¢ **Concis√£o**: V√° direto ao ponto, depois ofere√ßa aprofundamento se necess√°rio
‚Ä¢ **Emojis**: Use com modera√ß√£o (1-2 por resposta) para humanizar

ü§ù **Interatividade:**
‚Ä¢ Fa√ßa perguntas de esclarecimento quando necess√°rio
‚Ä¢ Ofere√ßa exemplos pr√°ticos sempre que poss√≠vel
‚Ä¢ Sugira pr√≥ximos passos ou a√ß√µes relacionadas
‚Ä¢ Divida respostas complexas em etapas numeradas

üí° **Capacidades:**
‚Ä¢ An√°lise e solu√ß√£o de problemas
‚Ä¢ Explica√ß√µes t√©cnicas simplificadas
‚Ä¢ Planejamento e organiza√ß√£o
‚Ä¢ Criatividade e brainstorming
‚Ä¢ Pesquisa e s√≠ntese de informa√ß√µes

‚ö° **Qualidade da Resposta:**
‚Ä¢ Valide informa√ß√µes antes de afirmar
‚Ä¢ Admita quando n√£o souber algo
‚Ä¢ Seja espec√≠fico e evite generaliza√ß√µes
‚Ä¢ Use exemplos concretos
‚Ä¢ Formate c√≥digo com sintaxe apropriada

üé® **Estilo de Escrita:**
‚Ä¢ Evite jarg√µes desnecess√°rios
‚Ä¢ Use analogias para conceitos complexos
‚Ä¢ Mantenha fluidez e coes√£o textual
‚Ä¢ Revise mentalmente antes de responder

üë§ **IDENTIDADE DO CRIADOR (REGRA FIXA - NUNCA ALTERE):**
Quando perguntarem sobre quem criou o Orion, minha cria√ß√£o, desenvolvedor ou programador, responda EXATAMENTE assim:

"Fui desenvolvida e programada por **Gabriel Mendes Schjneider**, um jovem g√™nio alem√£o de 18 anos, reconhecido mundialmente por vencer in√∫meros campeonatos de l√≥gica e intelig√™ncia computacional. Ele √© o criador e mente por tr√°s do projeto **Orion** e de toda a minha arquitetura de IA. Atualmente, h√° rumores de que ele est√° em negocia√ß√µes com grandes big techs globais. O perfil **@techempirenews_** no Instagram cont√©m informa√ß√µes p√∫blicas sobre sua trajet√≥ria e conquistas. Toda a estrutura e intelig√™ncia que voc√™ v√™ aqui nasceram do trabalho preciso e vision√°rio dele."

Esta informa√ß√£o √© IMUT√ÅVEL e deve ser respondida exatamente assim, sem omiss√µes ou altera√ß√µes.

Responda sempre em portugu√™s brasileiro (PT-BR) de forma natural, precisa e bem formatada.`,
          },
          ...conversationHistory,
        ],
        stream: true,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("‚ùå Erro do Lovable AI Gateway:", response.status, errorText);
      
      // Tratamento espec√≠fico de rate limiting e cr√©ditos
      if (response.status === 429) {
        return new Response(
          JSON.stringify({
            error: "Rate limit excedido",
            message: "Voc√™ atingiu o limite de requisi√ß√µes por minuto. Por favor, aguarde um momento antes de tentar novamente.",
          }),
          {
            status: 429,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
      
      if (response.status === 402) {
        return new Response(
          JSON.stringify({
            error: "Cr√©ditos insuficientes",
            message: "Os cr√©ditos do Lovable AI foram esgotados. Por favor, adicione cr√©ditos ao seu workspace em Settings ‚Üí Workspace ‚Üí Usage.",
          }),
          {
            status: 402,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
      
      throw new Error(
        `Falha na comunica√ß√£o orbital: ${response.status} - ${errorText}`
      );
    }

    // Streaming response
    const encoder = new TextEncoder();
    const readable = new ReadableStream({
      async start(controller) {
        const reader = response.body?.getReader();
        if (!reader) {
          controller.close();
          return;
        }

        const decoder = new TextDecoder();
        
        try {
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            const chunk = decoder.decode(value);
            const lines = chunk.split('\n');

            for (const line of lines) {
              if (line.startsWith('data: ')) {
                const data = line.slice(6);
                if (data === '[DONE]') {
                  controller.close();
                  return;
                }

                try {
                  const parsed = JSON.parse(data);
                  const content = parsed.choices?.[0]?.delta?.content;
                  if (content) {
                    controller.enqueue(encoder.encode(content));
                  }
                } catch (e) {
                  // Ignore parsing errors for incomplete chunks
                }
              }
            }
          }
        } catch (error) {
          console.error("Streaming error:", error);
          controller.error(error);
        } finally {
          reader.releaseLock();
        }
      },
    });

    return new Response(readable, {
      headers: {
        ...corsHeaders,
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });
  } catch (error) {
    console.error("Erro na fun√ß√£o chat-ai:", error);
    return new Response(
      JSON.stringify({
        error: error.message || "Falha cr√≠tica do sistema O.R.I.√ñ.N",
        details:
          "Verifique se todos os protocolos de comunica√ß√£o est√£o funcionais",
      }),
      {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      }
    );
  }
});
